# CIFAR10データセットを使った深層学習の勉強

## ノウハウ
* データの前処理は大事
  * データオーギュメンテーションを行わないと過学習しまくる
  * 加工がキツ過ぎても汎化性能は落ちる(上下反転、色を極端にいじるなど)
* MaxPoolはストライド幅2の畳み込みで代用できる
* 浅いネットワークではMaxPoolを全て置き換えても性能はほとんど落ちないが、深くなるとパラメータが収束しなくなる
  * したがって深いネットワークではMaxPoolを先頭に1つ挟む必要がある
  * なぜMaxPoolを入れると収束するのかはわからない(MaxPoolは最大出力をもつノード以外のノードに対する勾配の伝達をせき止めるから？)
* 重みの初期化は大事(ハマりポイント)
  * 浅いときは初期化せずとも収束するが、深くなるほど勾配が爆発しやすくなる
  * 学習が早くなる
* 学習が十分に進行するには一定のイテレーション数と学習率が必要
  * イテレーション数(バッチ数xエポック数)が足りなければ、そもそも重みが最適解まで移動できない
  * 学習率が低ければ、局所最適解に留まる可能性が高まる
* バッチサイズとイテレーション数は反比例するので、イテレーション数を保つ場合は、バッチサイズとエポック数は同じ割合で変化させる
* valロスの分散が大きくて不安定な時や過学習気味なときは、バッチサイズを増やして滑らかにする


## 汎化性能をあげるポイント
* データオーギュメンテーション(前処理)
* 最低限のモデルサイズ(深くしたいなら残差結合、重みの初期化は必須)
* ハイパーパラメータの調整
  * 学習率の逐次調整
  * イテレーション数
